{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from doctr.models import ocr_predictor\n",
    "\n",
    "\n",
    "def encode_image(image_path: str):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "class Pixtral:\n",
    "    def __init__(self, max_model_len=4096, max_tokens=2048, gpu_memory_utilization=0.65, temperature=0.35):\n",
    "        self.model_name = \"mistralai/Pixtral-12B-2409\"\n",
    "\n",
    "        self.sampling_params = SamplingParams(max_tokens=max_tokens, temperature=temperature)\n",
    "\n",
    "        self.llm = LLM(\n",
    "            model=self.model_name,\n",
    "            tokenizer_mode=\"mistral\",\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            load_format=\"mistral\",\n",
    "            config_format=\"mistral\",\n",
    "            max_model_len=max_model_len\n",
    "        )\n",
    "\n",
    "    def generate_message_from_image(self, prompt, image_path):\n",
    "        base64_image = encode_image(image_path)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        outputs = self.llm.chat(messages, sampling_params=self.sampling_params)\n",
    "\n",
    "        return outputs[0].outputs[0].text\n",
    "\n",
    "    def generate_message(self, prompt):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        outputs = self.llm.chat(messages, sampling_params=self.sampling_params)\n",
    "\n",
    "        return outputs[0].outputs[0].text\n",
    "\n",
    "class PIIPipeline:\n",
    "    def __init__(self, pixtral_model: Pixtral):\n",
    "        self.model = pixtral_model\n",
    "        self.ocr = ocr_predictor(pretrained=True)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def perform_ocr(ocr_model, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "        input_page = np.array(image)\n",
    "        result = ocr_model([input_page])\n",
    "        words = []\n",
    "        boxes = []\n",
    "        for page in result.pages:\n",
    "            for block in page.blocks:\n",
    "                for line in block.lines:\n",
    "                    for word in line.words:\n",
    "                        (rel_x0, rel_y0), (rel_x1, rel_y1) = word.geometry\n",
    "                        abs_x0 = int(rel_x0 * width)\n",
    "                        abs_y0 = int(rel_y0 * height)\n",
    "                        abs_x1 = int(rel_x1 * width)\n",
    "                        abs_y1 = int(rel_y1 * height)\n",
    "                        words.append(word.value)\n",
    "                        boxes.append([abs_x0, abs_y0, abs_x1, abs_y1])\n",
    "        return words, boxes\n",
    "\n",
    "    def process_image(self, image_path: str):\n",
    "\n",
    "        words, bboxes = self.perform_ocr(self.ocr, image_path)\n",
    "\n",
    "        word_data = [\n",
    "            {\"id\": idx, \"word\": word}\n",
    "            for idx, word in enumerate(words)\n",
    "        ]\n",
    "        bboxes_data = {\n",
    "            idx: bbox for idx, bbox in enumerate(bboxes)\n",
    "        }\n",
    "\n",
    "        input_json = json.dumps({\"words\": word_data}, ensure_ascii=False)\n",
    "\n",
    "        prompt = self.build_prompt(input_json)\n",
    "\n",
    "        response = self.model.generate_message_from_image(prompt, image_path)\n",
    "\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        \n",
    "        result = {\n",
    "            \"tokens\": [],\n",
    "            \"bboxes\": [],\n",
    "            \"ner_tags\": []\n",
    "        }\n",
    "        for entity in response[\"entities\"]:\n",
    "            word_id = entity[\"id\"]\n",
    "            if word_id in bboxes_data:\n",
    "                bbox = bboxes_data[word_id]\n",
    "                result[\"tokens\"].append(entity[\"word\"])\n",
    "                result[\"bboxes\"].append(bbox)\n",
    "                result[\"ner_tags\"].append(\"B-\" + entity[\"label\"])\n",
    "                \n",
    "        return result\n",
    "\n",
    "    def build_prompt(self, input_json: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are a PII information extraction model. You are given:\n",
    "\n",
    "1. An image of a scanned document (e.g., invoice, form, letter).\n",
    "2. A JSON object that contains the OCR result for this image. It consists of a list of tokens (words). Each token includes:\n",
    "   - \"id\": a unique ID for the token (integer)\n",
    "   - \"word\": the OCR-recognized text (may be a single character, digit, or symbol, like \"4\", \"@\" or \"-\")\n",
    "\n",
    "Your task:\n",
    "\n",
    "- Analyze the OCR result and the document image.\n",
    "- You should keep in mind that there may be OCR errors. Consider the context of the document and the meaning of the words.\n",
    "- Label each token that belongs to a PII (Personally Identifiable Information) entity with one of the following categories:\n",
    "\n",
    "\"full_name\" â€“ a personâ€™s full name, including first name, last name, and middle name (if applicable)\n",
    "\"phone_number\" â€“ phone number (including country codes, separators, etc.)  \n",
    "\"address\" â€“ any part of a physical address  \n",
    "\"email_address\" â€“ any part of an email address  \n",
    "\"company name\" â€“ name of a company or organization  \n",
    "\"payment_information\" â€“ IBAN or credit/debit card numbers\n",
    "\n",
    "ðŸ“Œ Notes:\n",
    "\n",
    "- You must only label tokens that belong to a PII entity. Do not return tokens labeled \"O\".\n",
    "- Each token should be returned along with its original \"id\" and assigned \"label\". Check that all the ids match the original input.\n",
    "- Some entities (like phone numbers or addresses) are composed of multiple tokens â€” e.g., a phone number \"+49 123 456789\" may consist of 3 tokens. Each token must be labeled \"phone_number\".\n",
    "- If a token is incorrect due to OCR but its intention is clear, you can still label it. For example, if the OCR result is \"Mex Mastermann's\" but the correct form is \"Max Mustermann's\", you should label both tokens as \"full_name\".\n",
    "\n",
    "Example Input:\n",
    "```json\n",
    "{{\n",
    "    \"words\": [\n",
    "        {{\"id\": 0, \"word\": \"Max\"}},\n",
    "        {{\"id\": 1, \"word\": \"Mustermann's\"}},\n",
    "        {{\"id\": 2, \"word\": \"phone\"}},\n",
    "        {{\"id\": 3, \"word\": \"number\"}},\n",
    "        {{\"id\": 4, \"word\": \"is\"}},\n",
    "        {{\"id\": 5, \"word\": \"+49\"}},\n",
    "        {{\"id\": 6, \"word\": \"123\"}},\n",
    "        {{\"id\": 7, \"word\": \"456789\"}}\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "\n",
    "âœ… Output format:\n",
    "```json\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\"id\": 0, \"word\": \"Max\", \"label\": \"full_name\"}},\n",
    "    {{\"id\": 1, \"word\": \"Mustermann\", \"label\": \"full_name\"}},\n",
    "    {{\"id\": 5, \"word\": \"+49\", \"label\": \"phone_number\"}},\n",
    "    {{\"id\": 6, \"word\": \"123\", \"label\": \"phone_number\"}},\n",
    "    {{\"id\": 7, \"word\": \"456789\", \"label\": \"phone_number\"}}\n",
    "  ]\n",
    "}}\n",
    "Now analyze the image and the OCR result provided below. Return only the JSON object WITHOUT ANY COMMENTS.\n",
    "\n",
    "OCR data:\n",
    "{input_json}\n",
    "\"\"\".strip()"
   ],
   "id": "40e8e5f2bf615e8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "images = os.listdir(\"data/funsd_benchmark/images\")\n",
    "labels_path = \"data/pixtral_funsd_benchmark/layoutlm_labels\"\n",
    "model = Pixtral()\n",
    "pipeline = PIIPipeline(model)    "
   ],
   "id": "686de5161de5ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for image in images:\n",
    "    if not images.endswith(\".png\"):\n",
    "        continue\n",
    "    image_path = os.path.join(\"data/funsd_benchmark/images\", image)\n",
    "    result = pipeline.process_image(image_path)\n",
    "    if result:\n",
    "        with open(os.path.join(labels_path, image.replace(\".png\", \".json\")), \"w\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(f\"No result returned for image: {image}\")"
   ],
   "id": "c05d2e75fb40b2e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6d8cf767a6d06e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:13:13.375721Z",
     "start_time": "2025-04-04T15:13:13.372015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_samples = [\n",
    "        {   \n",
    "            \"test_name\": \"benchmark\",\n",
    "            \"gt_labels\": \"data/funsd_benchmark/layoutlm_labels\",\n",
    "            \"predicted_labels\": \"data/pixtral_funsd_benchmark/layoutlm_labels\",\n",
    "            \"image_views\": \"data/pixtral_funsd_benchmark/labeled_images\",\n",
    "            \"class_names\": [\n",
    "                \"full_name\", \"phone_number\", \"address\", \"email_address\", \"company_name\"\n",
    "            ]\n",
    "        },\n",
    "]"
   ],
   "id": "de23d165064a3ed4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:13:37.350401Z",
     "start_time": "2025-04-04T15:13:16.815514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.w_b import count_and_log_all_metrics\n",
    "\n",
    "count_and_log_all_metrics(\n",
    "    samples=test_samples,\n",
    "    lm_model_name=f\"Pixtral\",\n",
    "    ocr_model_name=\"\",\n",
    "    run_specification=\"benchmark\"\n",
    ")"
   ],
   "id": "70f908c186bfb6c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malexandraroze2000\u001B[0m (\u001B[33malexandraroze\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/Aleksandra.Sorokovikova/Documents_anonymisation/workspace/document-anonymization/wandb/run-20250404_171322-0ig3m73d</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexandraroze/PII_Detection/runs/0ig3m73d' target=\"_blank\">Pixtral__benchmark</a></strong> to <a href='https://wandb.ai/alexandraroze/PII_Detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/alexandraroze/PII_Detection' target=\"_blank\">https://wandb.ai/alexandraroze/PII_Detection</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/alexandraroze/PII_Detection/runs/0ig3m73d' target=\"_blank\">https://wandb.ai/alexandraroze/PII_Detection/runs/0ig3m73d</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Pixtral__benchmark</strong> at: <a href='https://wandb.ai/alexandraroze/PII_Detection/runs/0ig3m73d' target=\"_blank\">https://wandb.ai/alexandraroze/PII_Detection/runs/0ig3m73d</a><br> View project at: <a href='https://wandb.ai/alexandraroze/PII_Detection' target=\"_blank\">https://wandb.ai/alexandraroze/PII_Detection</a><br>Synced 5 W&B file(s), 4 media file(s), 175 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_171322-0ig3m73d/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83f2d3df548a2de0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
